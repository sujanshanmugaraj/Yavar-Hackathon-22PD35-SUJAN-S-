# IMAGE-CAPTIONING-FROM-CONTEXTUAL-METADATA-USING-VISION-LANGUAGE-MODELS-VLMS

## ğŸŒ Overview

This project generates **concise** and **detailed** captions for visual content (charts, diagrams, images, etc.) using both:

* **Visual content** of the image
* **Contextual metadata** like section headers, surrounding text, captions, and footnotes

It leverages fine-tuned open-source Vision-Language Models (VLMs) to generate grounded and relevant captions with **confidence scores**, overlays the captions on the images, and evaluates the results using BLEU, ROUGE, and semantic similarity.

---

## ğŸ§° Problem Statement

Develop a fine-tuned open-source VLM that:

* Accepts an image and its metadata (text around the image)
* Generates a short **concise caption** and a longer **detailed caption**
* Evaluates them using standard NLP metrics
* Overlays the captions on the image with appropriate formatting

---

## ğŸ“š Folder Structure

```
IMAGE-CAPTIONING-FROM-CONTEXTUAL-METADATA-USING-VISION-LANGUAGE-MODELS-VLMS/
â”œâ”€â”€ backend/                      # Flask API backend
â”‚   â”œâ”€â”€ app.py                   
â”‚   â””â”€â”€ inference_pipeline.py   
â”œâ”€â”€ checkpoints/                # Fine-tuned model weights
â”œâ”€â”€ frontend/                   # HTML frontend interface
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ img_folder/                 # Input test images
â”œâ”€â”€ logs/                       # Logs for low confidence/inconsistencies
â”œâ”€â”€ metadata_folder/           # Metadata text files per image
â”œâ”€â”€ output_folder/             # Final annotated images and captions.json
â”œâ”€â”€ src/                        
â”‚   â”œâ”€â”€ caption_generator.py
â”‚   â”œâ”€â”€ consistency_checker.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ dataset_loader.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ generate_caption_with_confidence.py
â”‚   â”œâ”€â”€ image_overlay.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ metadata_parser.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ generate_synthetic_metadata.py
â”‚   â”œâ”€â”€ merge_image_datasets_balanced.py
â”‚   â”œâ”€â”€ generate_dummy_captions.py
â”‚   â””â”€â”€ image_resolver.py
â”œâ”€â”€ captions.json              # Stores all caption results
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md
â”œâ”€â”€ test_caption_generator.py
â”œâ”€â”€ test_image_loader.py
```

---

## ğŸ¤– Input Format

### Images

All test images (charts, graphs, circuit diagrams) should be placed in `img_folder/`.

### Metadata

For each image, a `.txt` file with the same name (e.g. `image1.jpg` -> `image1.txt`) must be in `metadata_folder/`. Format:

```
section_header: Performance Trends
above_text: Market distribution
caption: Revenue distribution by region
picture_id: #/pictures/0
footnote: Generated by AI system
below_text: Source - Internal Report
```

---

## ğŸ“Š Output Format

In `output_folder/`:

* **Annotated Image:** Image with overlaid captions (concise in blue, detailed in red)
* **captions.json:**

```json
{
  "image1.jpg": {
    "concise_caption": {
      "text": "revenue by region",
      "confidence": 0.92
    },
    "detailed_caption": {
      "text": "this chart shows revenue split by region, with asia pacific leading the share",
      "confidence": 0.89
    },
    "metadata": { ... }
  }
}
```

---

## ğŸš€ Model & Methodology

### ğŸŒŸ VLM Used

* BLIP-2 with ViT-GGML architecture
* Fine-tuned on our custom dataset combining image content and structured metadata

### ğŸš€ Training

Training is performed using the [`train.py`](./train.py) script.

#### ğŸ‹ï¸â€â™‚ï¸ Training Process

- The dataset is composed of:
  - Chart images from [`img_folder/`](./img_folder/)
  - Corresponding metadata in [`metadata_folder/`](./metadata_folder/)
  - Human-annotated captions from [`captions.json`](./utils/captions.json)
  
- Fine-tuning is done on image-metadata-caption triplets.
- Default setup uses a subset of 30 samples (`--subset_size 30`) for quick testing.
- Loss and token-level accuracy are logged during each epoch.
- Trained model checkpoints are saved to the [`checkpoints/`](./checkpoints/) directory.

#### âš™ï¸ Configuration

- **Optimizer:** AdamW  
- **Epochs:** 3  
- **Batch Size:** 2  
- **Learning Rate:** 5e-5  
- **Scheduler:** Linear (no warmup)

![Screenshot 2025-05-29 183525](https://github.com/user-attachments/assets/bb2d4415-ac21-4259-a9a5-c6ef809f3dff)



### ğŸ–¼ï¸ Images Considered (These are a few of them)

![Screenshot 2025-05-30 232106](https://github.com/user-attachments/assets/a32f1def-f9be-426d-9a42-2e89746cb9d4)
![Screenshot 2025-05-30 232140](https://github.com/user-attachments/assets/82e01f17-069f-4499-8ff7-c130e3bbc473)
![Screenshot 2025-05-30 232202](https://github.com/user-attachments/assets/cc4b8da8-5b48-40a4-905e-bb0d52313918)
![Screenshot 2025-05-30 232230](https://github.com/user-attachments/assets/88a8a7c1-ff37-4683-b5ab-02ba332b168a)
![Screenshot 2025-05-30 232253](https://github.com/user-attachments/assets/5c98b00d-da5c-4959-83e2-7833bdedf2e9)
![Screenshot 2025-05-30 232314](https://github.com/user-attachments/assets/8d70afeb-3a8a-4e55-9c3f-f4644c7e6686)



### ğŸ‘¨â€ğŸ“‹ Preprocessing

* Image resizing & normalization
* Metadata parsed and tokenized (using `metadata_parser.py`)

### ğŸŒ Caption Generation

* `generate_caption_with_confidence.py` combines:

  * Visual features (from ViT)
  * Contextual embedding (metadata)
* Generates:

  * `Concise Caption`: Summary-style
  * `Detailed Caption`: Descriptive, explanatory

### âœ… Evaluation Metrics

* BLEU
* ROUGE-1, ROUGE-L
* Semantic similarity using Sentence-BERT cosine score
* Implemented in `evaluation.py`

### âš¡ Confidence & Consistency

* Each caption has a score \[0.0 - 1.0]
* If score < threshold (e.g., 0.4), it is highlighted in overlay
* `consistency_checker.py` logs contradictions with metadata

---

## ğŸŒ Frontend & API

* `frontend/index.html` allows image upload
* Connects to `backend/app.py` Flask server
* Displays:

  * Parsed metadata
  * Generated captions + scores
  * Annotated image

---

## âœï¸ Logs & Error Handling

* Low-confidence or contradictory captions logged in `logs/`
* Invalid images or corrupted metadata files are gracefully handled

---

## ğŸ“‰ Example Output

### The input image

![ds1_bar_chart_images399](https://github.com/user-attachments/assets/8dc36fac-eb43-48a0-8f19-62c8c3f445fa)

### Frontend

![Screenshot 2025-05-30 192349](https://github.com/user-attachments/assets/662df9c8-2253-44c5-a3a0-a6174064156f)

![Screenshot (281)](https://github.com/user-attachments/assets/65f26bf4-c760-4019-8765-2145560d7df4)

![Screenshot (282)](https://github.com/user-attachments/assets/2298e83a-410e-4019-9556-b818c9e40098)

![WhatsApp Image 2025-05-30 at 23 06 14_90805819](https://github.com/user-attachments/assets/bfe36a75-c4af-40d1-8489-241260d4b11c)

![Screenshot 2025-05-30 210623](https://github.com/user-attachments/assets/932763b6-6ec0-40d6-8426-95a93e4bf4b8)


* **Concise (92%)**: "ğŸ“ LeBron James leads endorsement earnings among top athletes"
* **Detailed (89%)**: "ğŸ“‹ The chart shows that LeBron James earns significantly more from endorsements (\$48M) than from sports (\$19M)..."

Annotated in:

* Blue (concise)
* Red (detailed)

---

## ğŸšª Constraints

* Only open-source models/libraries used
* Mandatory fine-tuning done
* Robust evaluation and logging pipeline

---

