# IMAGE-CAPTIONING-FROM-CONTEXTUAL-METADATA-USING-VISION-LANGUAGE-MODELS-VLMS

## Author 

**SUJAN S (22PD35)**  
MSc (Data Science)

## Overview

This project generates **concise** and **detailed** captions for visual content (charts, diagrams, images, etc.) using both:

* **Visual content** of the image
* **Contextual metadata** like section headers, surrounding text, captions, and footnotes

It leverages fine-tuned open-source Vision-Language Models (VLMs) to generate grounded and relevant captions with **confidence scores**, overlays the captions on the images, and evaluates the results using BLEU, ROUGE, and semantic similarity.

---

## Problem Statement

Develop a fine-tuned open-source VLM that:

* Accepts an image and its metadata (text around the image)
* Generates a short **concise caption** and a longer **detailed caption**
* Evaluates them using standard NLP metrics
* Overlays the captions on the image with appropriate formatting

---

## Folder Structure

```
IMAGE-CAPTIONING-FROM-CONTEXTUAL-METADATA-USING-VISION-LANGUAGE-MODELS-VLMS/
├── backend/                      # Flask API backend
│   ├── app.py                   
│   └── inference_pipeline.py   
├── checkpoints/                # Fine-tuned model weights
├── frontend/                   # HTML frontend interface
│   └── index.html
├── img_folder/                 # Input test images
├── logs/                       # Logs for low confidence/inconsistencies
├── metadata_folder/           # Metadata text files per image
├── output_folder/             # Final annotated images and captions.json
├── src/                        
│   ├── caption_generator.py
│   ├── consistency_checker.py
│   ├── data_loader.py
│   ├── dataset_loader.py
│   ├── evaluation.py
│   ├── generate_caption_with_confidence.py
│   ├── image_overlay.py
│   ├── logger.py
│   ├── metadata_parser.py
│   ├── model_loader.py
│   ├── preprocessor.py
│   ├── train.py
│   └── utils.py
├── utils/
│   ├── generate_synthetic_metadata.py
│   ├── merge_image_datasets_balanced.py
│   ├── generate_dummy_captions.py
│   └── image_resolver.py
├── captions.json              # Stores all caption results
├── requirements.txt           # Python dependencies
├── README.md
├── test_caption_generator.py
├── test_image_loader.py
```

---

## Input Format

### Images

All test images (charts, graphs, circuit diagrams) should be placed in `img_folder/`.

### Metadata

For each image, a `.txt` file with the same name (e.g. `image1.jpg` -> `image1.txt`) must be in `metadata_folder/`. Format:

```
section_header: Performance Trends
above_text: Market distribution
caption: Revenue distribution by region
picture_id: #/pictures/0
footnote: Generated by AI system
below_text: Source - Internal Report
```

---

## Output Format

In `output_folder/`:

* **Annotated Image:** Image with overlaid captions (concise in blue, detailed in red)
* **captions.json:**

```json
{
  "image1.jpg": {
    "concise_caption": {
      "text": "revenue by region",
      "confidence": 0.92
    },
    "detailed_caption": {
      "text": "this chart shows revenue split by region, with asia pacific leading the share",
      "confidence": 0.89
    },
    "metadata": { ... }
  }
}
```

---

## Model & Methodology

### VLM Used

* BLIP-2 with ViT-GGML architecture
* Fine-tuned on my custom dataset combining image content and structured metadata

### Training

Training is performed using the [`train.py`](./train.py) script.

#### Training Process

- The dataset is composed of:
  - Chart images from [`img_folder/`](./img_folder/)
  - Corresponding metadata in [`metadata_folder/`](./metadata_folder/)
  - Human-annotated captions from [`captions.json`](./utils/captions.json)
  
- Fine-tuning is done on image-metadata-caption triplets.
- Default setup uses a subset of 30 samples (`--subset_size 30`) for quick testing.
- Loss and token-level accuracy are logged during each epoch.
- Trained model checkpoints are saved to the [`checkpoints/`](./checkpoints/) directory.

#### Configuration

- **Optimizer:** AdamW  
- **Epochs:** 3  
- **Batch Size:** 2  
- **Learning Rate:** 5e-5  
- **Scheduler:** Linear (no warmup)

![Screenshot 2025-05-29 183525](https://github.com/user-attachments/assets/bb2d4415-ac21-4259-a9a5-c6ef809f3dff)



### Images Considered (These are a few of them)

![Screenshot 2025-05-30 232106](https://github.com/user-attachments/assets/a32f1def-f9be-426d-9a42-2e89746cb9d4)
![Screenshot 2025-05-30 232202](https://github.com/user-attachments/assets/cc4b8da8-5b48-40a4-905e-bb0d52313918)
![Screenshot 2025-05-30 232230](https://github.com/user-attachments/assets/88a8a7c1-ff37-4683-b5ab-02ba332b168a)
![Screenshot 2025-05-30 232253](https://github.com/user-attachments/assets/5c98b00d-da5c-4959-83e2-7833bdedf2e9)
![Screenshot 2025-05-30 232314](https://github.com/user-attachments/assets/8d70afeb-3a8a-4e55-9c3f-f4644c7e6686)



### Preprocessing

* Image resizing & normalization
* Metadata parsed and tokenized (using `metadata_parser.py`)

### Caption Generation

* `generate_caption_with_confidence.py` combines:

  * Visual features (from ViT)
  * Contextual embedding (metadata)
* Generates:

  * `Concise Caption`: Summary-style
  * `Detailed Caption`: Descriptive, explanatory

### Evaluation Metrics

* BLEU
* ROUGE-1, ROUGE-L
* Semantic similarity using Sentence-BERT cosine score
* Implemented in `evaluation.py`

### Confidence & Consistency

* Each caption has a score \[0.0 - 1.0]
* If score < threshold (e.g., 0.4), it is highlighted in overlay
* `consistency_checker.py` logs contradictions with metadata

---

## Frontend & API

* `frontend/index.html` allows image upload
* Connects to `backend/app.py` Flask server
* Displays:

  * Parsed metadata
  * Generated captions + scores
  * Annotated image

---

## Logs & Error Handling

* Low-confidence or contradictory captions logged in `logs/`
* Invalid images or corrupted metadata files are gracefully handled

---

## Example Output

### The input image

![ds1_bar_chart_images399](https://github.com/user-attachments/assets/8dc36fac-eb43-48a0-8f19-62c8c3f445fa)

### Frontend

![Screenshot 2025-05-30 192349](https://github.com/user-attachments/assets/a7a3a225-7ab7-4c26-8237-9dbdce7b7310)

![Screenshot 2025-05-30 210623](https://github.com/user-attachments/assets/e637a94c-6c2a-485f-ae9d-207ffbfa928e)

![Screenshot 2025-05-30 210632](https://github.com/user-attachments/assets/5dbe54c2-ae77-48c2-b0b7-a085ba3c23f1)

![WhatsApp Image 2025-05-30 at 23 06 14_90805819](https://github.com/user-attachments/assets/bfe36a75-c4af-40d1-8489-241260d4b11c)

![Screenshot 2025-05-30 210649](https://github.com/user-attachments/assets/697e28e4-abc6-4001-b994-214ff063945d)


* **Concise (92%)**: "LeBron James leads endorsement earnings among top athletes"
* **Detailed (89%)**: "The chart shows that LeBron James earns significantly more from endorsements (\$48M) than from sports (\$19M)..."

Annotated in:

* Blue (concise)
* Red (detailed)

---

## Constraints

* Only open-source models/libraries used
* Mandatory fine-tuning done
* Robust evaluation and logging pipeline

---

