# IMAGE-CAPTIONING-FROM-CONTEXTUAL-METADATA-USING-VISION-LANGUAGE-MODELS-VLMS

## ğŸŒ Overview

This project generates **concise** and **detailed** captions for visual content (charts, diagrams, images, etc.) using both:

* **Visual content** of the image
* **Contextual metadata** like section headers, surrounding text, captions, and footnotes

It leverages fine-tuned open-source Vision-Language Models (VLMs) to generate grounded and relevant captions with **confidence scores**, overlays the captions on the images, and evaluates the results using BLEU, ROUGE, and semantic similarity.

---

## ğŸ§° Problem Statement

Develop a fine-tuned open-source VLM that:

* Accepts an image and its metadata (text around the image)
* Generates a short **concise caption** and a longer **detailed caption**
* Evaluates them using standard NLP metrics
* Overlays the captions on the image with appropriate formatting

---

## ğŸ“š Folder Structure

```
IMAGE-CAPTIONING-FROM-CONTEXTUAL-METADATA-USING-VISION-LANGUAGE-MODELS-VLMS/
â”œâ”€â”€ backend/                      # Flask API backend
â”‚   â”œâ”€â”€ app.py                   
â”‚   â””â”€â”€ inference_pipeline.py   
â”œâ”€â”€ checkpoints/                # Fine-tuned model weights
â”œâ”€â”€ frontend/                   # HTML frontend interface
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ img_folder/                 # Input test images
â”œâ”€â”€ logs/                       # Logs for low confidence/inconsistencies
â”œâ”€â”€ metadata_folder/           # Metadata text files per image
â”œâ”€â”€ output_folder/             # Final annotated images and captions.json
â”œâ”€â”€ src/                        
â”‚   â”œâ”€â”€ caption_generator.py
â”‚   â”œâ”€â”€ consistency_checker.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ dataset_loader.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ generate_caption_with_confidence.py
â”‚   â”œâ”€â”€ image_overlay.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ metadata_parser.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ generate_synthetic_metadata.py
â”‚   â”œâ”€â”€ merge_image_datasets_balanced.py
â”‚   â”œâ”€â”€ generate_dummy_captions.py
â”‚   â””â”€â”€ image_resolver.py
â”œâ”€â”€ captions.json              # Stores all caption results
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md
â”œâ”€â”€ test_caption_generator.py
â”œâ”€â”€ test_image_loader.py
```

---

## ğŸ¤– Input Format

### Images

All test images (charts, graphs, circuit diagrams) should be placed in `img_folder/`.

### Metadata

For each image, a `.txt` file with the same name (e.g. `image1.jpg` -> `image1.txt`) must be in `metadata_folder/`. Format:

```
section_header: Performance Trends
above_text: Market distribution
caption: Revenue distribution by region
picture_id: #/pictures/0
footnote: Generated by AI system
below_text: Source - Internal Report
```

---

## ğŸ“Š Output Format

In `output_folder/`:

* **Annotated Image:** Image with overlaid captions (concise in blue, detailed in red)
* **captions.json:**

```json
{
  "image1.jpg": {
    "concise_caption": {
      "text": "revenue by region",
      "confidence": 0.92
    },
    "detailed_caption": {
      "text": "this chart shows revenue split by region, with asia pacific leading the share",
      "confidence": 0.89
    },
    "metadata": { ... }
  }
}
```

---

## ğŸš€ Model & Methodology

### ğŸŒŸ VLM Used

* BLIP-2 with ViT-GGML architecture
* Fine-tuned on our custom dataset combining image content and structured metadata

### ğŸš€ Training

Training is performed using the [`train.py`](./train.py) script.

#### ğŸ‹ï¸â€â™‚ï¸ Training Process

- The dataset is composed of:
  - Chart images from [`img_folder/`](./img_folder/)
  - Corresponding metadata in [`metadata_folder/`](./metadata_folder/)
  - Human-annotated captions from [`captions.json`](./utils/captions.json)
  
- Fine-tuning is done on image-metadata-caption triplets.
- Default setup uses a subset of 30 samples (`--subset_size 30`) for quick testing.
- Loss and token-level accuracy are logged during each epoch.
- Trained model checkpoints are saved to the [`checkpoints/`](./checkpoints/) directory.

#### âš™ï¸ Configuration

- **Optimizer:** AdamW  
- **Epochs:** 3  
- **Batch Size:** 2  
- **Learning Rate:** 5e-5  
- **Scheduler:** Linear (no warmup)

![Screenshot 2025-05-29 183525](https://github.com/user-attachments/assets/b5f8ff8d-4efe-498c-8a50-c625d19dacdd)

### ğŸ–¼ï¸ Images Considered (These are a few of them)

![image](https://github.com/user-attachments/assets/04b6ad8b-53c4-49e0-92c1-047245a27bef)
![image](https://github.com/user-attachments/assets/7cd9cad2-3c90-4b18-91c9-de483f3b810a)
![image](https://github.com/user-attachments/assets/15cbe5bb-50a5-4f35-bc26-0b1c104f6f7a)
![image](https://github.com/user-attachments/assets/4700e97c-af32-49f9-9ef8-83c797735b1d)
![image](https://github.com/user-attachments/assets/48171cbf-6e87-493f-b1cd-8cce8e89c357)
![image](https://github.com/user-attachments/assets/a2235fb5-97d2-4d69-a8b7-025b3f33497c)




### ğŸ‘¨â€ğŸ“‹ Preprocessing

* Image resizing & normalization
* Metadata parsed and tokenized (using `metadata_parser.py`)

### ğŸŒ Caption Generation

* `generate_caption_with_confidence.py` combines:

  * Visual features (from ViT)
  * Contextual embedding (metadata)
* Generates:

  * `Concise Caption`: Summary-style
  * `Detailed Caption`: Descriptive, explanatory

### âœ… Evaluation Metrics

* BLEU
* ROUGE-1, ROUGE-L
* Semantic similarity using Sentence-BERT cosine score
* Implemented in `evaluation.py`

### âš¡ Confidence & Consistency

* Each caption has a score \[0.0 - 1.0]
* If score < threshold (e.g., 0.4), it is highlighted in overlay
* `consistency_checker.py` logs contradictions with metadata

---

## ğŸŒ Frontend & API

* `frontend/index.html` allows image upload
* Connects to `backend/app.py` Flask server
* Displays:

  * Parsed metadata
  * Generated captions + scores
  * Annotated image

---

## âœï¸ Logs & Error Handling

* Low-confidence or contradictory captions logged in `logs/`
* Invalid images or corrupted metadata files are gracefully handled

---

## ğŸ“‰ Example Output

### The input image

![ds1_bar_chart_images399](https://github.com/user-attachments/assets/27ac3650-985d-4bc0-8936-77c11ab9e1c6)

### Frontend

![Screenshot 2025-05-30 192349](https://github.com/user-attachments/assets/66a6f337-ac31-42ea-abad-b0776fcdc4bb)

![Screenshot 2025-05-30 210623](https://github.com/user-attachments/assets/43431518-2e78-4e62-9ab5-8d233aa05a38)

![Screenshot 2025-05-30 210632](https://github.com/user-attachments/assets/df9eab0b-1308-4853-be9a-80fe640abbff)

![WhatsApp Image 2025-05-30 at 23 06 15_926fd978](https://github.com/user-attachments/assets/64246deb-18ff-478f-921e-7f715c55d7bb)

![Screenshot 2025-05-30 210649](https://github.com/user-attachments/assets/b53150d5-0141-4464-bcbb-37262afd05c1)


* **Concise (92%)**: "ğŸ“ LeBron James leads endorsement earnings among top athletes"
* **Detailed (89%)**: "ğŸ“‹ The chart shows that LeBron James earns significantly more from endorsements (\$48M) than from sports (\$19M)..."

Annotated in:

* Blue (concise)
* Red (detailed)

---

## ğŸšª Constraints

* Only open-source models/libraries used
* Mandatory fine-tuning done
* Robust evaluation and logging pipeline

---

