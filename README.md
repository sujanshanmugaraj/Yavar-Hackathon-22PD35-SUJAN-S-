# IMAGE-CAPTIONING-FROM-CONTEXTUAL-METADATA-USING-VISION-LANGUAGE-MODELS-VLMS

## 🌐 Overview

This project generates **concise** and **detailed** captions for visual content (charts, diagrams, images, etc.) using both:

* **Visual content** of the image
* **Contextual metadata** like section headers, surrounding text, captions, and footnotes

It leverages fine-tuned open-source Vision-Language Models (VLMs) to generate grounded and relevant captions with **confidence scores**, overlays the captions on the images, and evaluates the results using BLEU, ROUGE, and semantic similarity.

---

## 🧰 Problem Statement

Develop a fine-tuned open-source VLM that:

* Accepts an image and its metadata (text around the image)
* Generates a short **concise caption** and a longer **detailed caption**
* Evaluates them using standard NLP metrics
* Overlays the captions on the image with appropriate formatting

---

## 📚 Folder Structure

```
IMAGE-CAPTIONING-FROM-CONTEXTUAL-METADATA-USING-VISION-LANGUAGE-MODELS-VLMS/
├── backend/                      # Flask API backend
│   ├── app.py                   
│   └── inference_pipeline.py   
├── checkpoints/                # Fine-tuned model weights
├── frontend/                   # HTML frontend interface
│   └── index.html
├── img_folder/                 # Input test images
├── logs/                       # Logs for low confidence/inconsistencies
├── metadata_folder/           # Metadata text files per image
├── output_folder/             # Final annotated images and captions.json
├── src/                        
│   ├── caption_generator.py
│   ├── consistency_checker.py
│   ├── data_loader.py
│   ├── dataset_loader.py
│   ├── evaluation.py
│   ├── generate_caption_with_confidence.py
│   ├── image_overlay.py
│   ├── logger.py
│   ├── metadata_parser.py
│   ├── model_loader.py
│   ├── preprocessor.py
│   ├── train.py
│   └── utils.py
├── utils/
│   ├── generate_synthetic_metadata.py
│   ├── merge_image_datasets_balanced.py
│   ├── generate_dummy_captions.py
│   └── image_resolver.py
├── captions.json              # Stores all caption results
├── requirements.txt           # Python dependencies
├── README.md
├── test_caption_generator.py
├── test_image_loader.py
```

---

## 🤖 Input Format

### Images

All test images (charts, graphs, circuit diagrams) should be placed in `img_folder/`.

### Metadata

For each image, a `.txt` file with the same name (e.g. `image1.jpg` -> `image1.txt`) must be in `metadata_folder/`. Format:

```
section_header: Performance Trends
above_text: Market distribution
caption: Revenue distribution by region
picture_id: #/pictures/0
footnote: Generated by AI system
below_text: Source - Internal Report
```

---

## 📊 Output Format

In `output_folder/`:

* **Annotated Image:** Image with overlaid captions (concise in blue, detailed in red)
* **captions.json:**

```json
{
  "image1.jpg": {
    "concise_caption": {
      "text": "revenue by region",
      "confidence": 0.92
    },
    "detailed_caption": {
      "text": "this chart shows revenue split by region, with asia pacific leading the share",
      "confidence": 0.89
    },
    "metadata": { ... }
  }
}
```

---

## 🚀 Model & Methodology

### 🌟 VLM Used

* BLIP-2 with ViT-GGML architecture
* Fine-tuned on our custom dataset combining image content and structured metadata

### 🚀 Training

Training is performed using the [`train.py`](./train.py) script.

#### 🏋️‍♂️ Training Process

- The dataset is composed of:
  - Chart images from [`img_folder/`](./img_folder/)
  - Corresponding metadata in [`metadata_folder/`](./metadata_folder/)
  - Human-annotated captions from [`captions.json`](./utils/captions.json)
  
- Fine-tuning is done on image-metadata-caption triplets.
- Default setup uses a subset of 30 samples (`--subset_size 30`) for quick testing.
- Loss and token-level accuracy are logged during each epoch.
- Trained model checkpoints are saved to the [`checkpoints/`](./checkpoints/) directory.

#### ⚙️ Configuration

- **Optimizer:** AdamW  
- **Epochs:** 3  
- **Batch Size:** 2  
- **Learning Rate:** 5e-5  
- **Scheduler:** Linear (no warmup)

![Screenshot 2025-05-29 183525](https://github.com/user-attachments/assets/b5f8ff8d-4efe-498c-8a50-c625d19dacdd)

### 🖼️ Images Considered (These are a few of them)

![image](https://github.com/user-attachments/assets/04b6ad8b-53c4-49e0-92c1-047245a27bef)
![image](https://github.com/user-attachments/assets/7cd9cad2-3c90-4b18-91c9-de483f3b810a)
![image](https://github.com/user-attachments/assets/15cbe5bb-50a5-4f35-bc26-0b1c104f6f7a)
![image](https://github.com/user-attachments/assets/4700e97c-af32-49f9-9ef8-83c797735b1d)
![image](https://github.com/user-attachments/assets/48171cbf-6e87-493f-b1cd-8cce8e89c357)
![image](https://github.com/user-attachments/assets/a2235fb5-97d2-4d69-a8b7-025b3f33497c)




### 👨‍📋 Preprocessing

* Image resizing & normalization
* Metadata parsed and tokenized (using `metadata_parser.py`)

### 🌐 Caption Generation

* `generate_caption_with_confidence.py` combines:

  * Visual features (from ViT)
  * Contextual embedding (metadata)
* Generates:

  * `Concise Caption`: Summary-style
  * `Detailed Caption`: Descriptive, explanatory

### ✅ Evaluation Metrics

* BLEU
* ROUGE-1, ROUGE-L
* Semantic similarity using Sentence-BERT cosine score
* Implemented in `evaluation.py`

### ⚡ Confidence & Consistency

* Each caption has a score \[0.0 - 1.0]
* If score < threshold (e.g., 0.4), it is highlighted in overlay
* `consistency_checker.py` logs contradictions with metadata

---

## 🌍 Frontend & API

* `frontend/index.html` allows image upload
* Connects to `backend/app.py` Flask server
* Displays:

  * Parsed metadata
  * Generated captions + scores
  * Annotated image

---

## ✍️ Logs & Error Handling

* Low-confidence or contradictory captions logged in `logs/`
* Invalid images or corrupted metadata files are gracefully handled

---

## 📉 Example Output

### The input image

![ds1_bar_chart_images399](https://github.com/user-attachments/assets/27ac3650-985d-4bc0-8936-77c11ab9e1c6)

### Frontend

![Screenshot 2025-05-30 192349](https://github.com/user-attachments/assets/66a6f337-ac31-42ea-abad-b0776fcdc4bb)

![Screenshot 2025-05-30 210623](https://github.com/user-attachments/assets/43431518-2e78-4e62-9ab5-8d233aa05a38)

![Screenshot 2025-05-30 210632](https://github.com/user-attachments/assets/df9eab0b-1308-4853-be9a-80fe640abbff)

![WhatsApp Image 2025-05-30 at 23 06 15_926fd978](https://github.com/user-attachments/assets/64246deb-18ff-478f-921e-7f715c55d7bb)

![Screenshot 2025-05-30 210649](https://github.com/user-attachments/assets/b53150d5-0141-4464-bcbb-37262afd05c1)


* **Concise (92%)**: "📝 LeBron James leads endorsement earnings among top athletes"
* **Detailed (89%)**: "📋 The chart shows that LeBron James earns significantly more from endorsements (\$48M) than from sports (\$19M)..."

Annotated in:

* Blue (concise)
* Red (detailed)

---

## 🚪 Constraints

* Only open-source models/libraries used
* Mandatory fine-tuning done
* Robust evaluation and logging pipeline

---

